# this is the main code for maitenance policy searching by RL
import numpy as np
import os, time, copy, pickle
from bridgedeterioration import BridgeEnv
from utils import getreturn, experience_buffer, time_encoder

num_component = 7
env = BridgeEnv()
episodes = 1000
gamma = 0.95


def generate_Q():

    if not os.path.exists('./result/simulation/'):
        os.mkdir('./result/simulation/')

    "Condition based policies"
    # conditon-1
    print('####  experience generated by condition-1 policy  #####')
    costs = []
    Q_dict = {}
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            a = np.zeros(num_component, dtype=np.int32)
            a[np.where(s > 1)] = 1  # condition-1
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/c-1/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)

    # condition-2
    print('####  experience generated by condition-2 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            a = np.zeros(num_component, dtype=np.int32)
            a[np.where(s > 2)] = 1  # condition-1
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/c-2/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)


    # condition-3
    print('####  experience generated by condition-3 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            rnd = np.random.rand()
            a = np.zeros(num_component, dtype=np.int32)
            a[np.where(s > 3)] = 1  # condition-1
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/c-3/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)

    "Time based policies"
    # time-5
    print('####  experience generated by time-5 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            rnd = np.random.rand()
            a = np.zeros(num_component, dtype=np.int32)
            if t%5==0 and t>0:
                a = np.ones(num_component, dtype=np.int32)
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/t-5/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)

    # time-10
    print('####  experience generated by time-10 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            rnd = np.random.rand()
            a = np.zeros(num_component, dtype=np.int32)
            if t%10==0 and t>0:
                a = np.ones(num_component, dtype=np.int32)
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/t-10/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)


    # time-15
    print('####  experience generated by time-15 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            rnd = np.random.rand()
            a = np.zeros(num_component, dtype=np.int32)
            if t%15==0 and t>0:
                a = np.ones(num_component, dtype=np.int32)
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/t-15/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)


    # time-2
    print('####  experience generated by time-20 policy  #####')
    costs = []
    actions = np.zeros([episodes, 100, num_component], dtype=np.int32)
    states = np.zeros([episodes, 100, num_component], dtype=np.int32)
    for i in range(episodes):
        _ = env.reset()
        s = env.state_num[:num_component]
        done = False
        t, rAll = 0, 0
        while not done:
            rnd = np.random.rand()
            a = np.zeros(num_component, dtype=np.int32)
            if t%20==0 and t>0:
                a = np.ones(num_component, dtype=np.int32)
            states[i, t, :] = copy.deepcopy(s)
            actions[i, t, :] = copy.deepcopy(a)
            state1, reward, done = env.step(a)
            r = reward / 600
            s1 = env.state_num[:num_component]
            s = s1
            t += 1
            # year = time_encoder(t, 7)[np.newaxis, np.newaxis, :]
            rAll += np.sum(r)
        print(rAll)
        costs.append(rAll)

    path = './result/simulation/t-20/'
    if not os.path.exists(path):
        os.mkdir(path)
    np.save(path + 'cost.npy', costs)
    np.save(path + 'states.npy', states)
    np.save(path + 'actions.npy', actions)

    with open('./result/Qvalue.pickle', 'wb') as saved:
        pickle.dump(Q_dict, saved)
